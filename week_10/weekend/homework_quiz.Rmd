---
title: "R Notebook"
output: html_notebook
---

# Homework Quiz

## Question 1

I want to predict how well 6 year-olds are going to do in their final school 
exams. Using the following variables am I likely under-fitting, fitting well or 
over-fitting? Postcode, gender, reading level, score in maths test, date of 
birth, family income.

__My Solution__

Based on the variables I would suggest you are over-fitting your model. It is
quite likely that variables such `Postcode`, `date of birth` and potentially
`family income` may not have much predictive power towards final school exams.

## Question 2

If I have two models, one with an AIC score of 34,902 and the other with an AIC 
score of 33,559 which model should I use?

__My Solution__

Based on those two AIC scores (and no other information) I would choose the
model with the _AIC score of 33,559_. This is since lower AIC scores indicate
better fitting models. So the model is 33,559 will be a better fit model over
the model with an AIC score of 34,902.

## Question 3

I have two models, the first with: r-squared: 0.44, adjusted r-squared: 0.43. 
The second with: r-squared: 0.47, adjusted r-squared: 0.41. Which one should I 
use?

__My Solution__

It depends. We would also like to take into account the number of variables
in each model, the AIC/BIC scores, results from potential ANOVA between models.
I would choose the first model with adjusted r-squared of 0.43 for three main
reasons:

* It has a greater adjusted r-squared, thus, does a better a job job of
explaining the response variable from our predcitor variables.
* The adjusted r-squared adjusts its score based on the number of predictors in
the model that are not significant. Penalising models with greater numbers of
predictors.
* The second model decreases to a greater extent from multiple - adjusted 
r-squared indicating it may have been penalised for having too many
insignificant variables that make it over-fit.

## Question 4

I have a model with the following errors: RMSE error on test set: 10.3, RMSE 
error on training data: 10.4. Do you think this model is over-fitting?

__My Solution__

No, I don't believe it is over-fitting. This is since the RMSE on the test set
is _less_ than the RMSE in the training data. If RMSE test > RMSE training, then
I would say it is over-fit.

## Question 5

How does k-fold validation work?

__My Solution__

K-fold validation works in the following steps:

* Decide how many equal subgroups (folds) you would like to split the data into
  - Usually a number between 5 - 10, let's say we choose 5.
  - This should be done as randomly as possible to ensure each fold is 
  representative of one another and the data as a whole.
* Treat one fold as your training data set and the rest as the test data
* Create your model using the fold chosen as the training data, then test your
model on the other 4 folds that are the test data.
* Repeat above process until each fold has been chosen as the test data set and
you have created an optimal model based on each fold.
* Once all 5 models have been created we average the error across all the test 
folds to determine the performance of our model.

## Question 6

What is a validation set? When do you need one?

__My Solution__

Ideally before any manipulation data should be split up into training,
validation and test sets. A validation set is there to provide an unbiased 
evaluation of a model fit on the training dataset while tuning model 
hyperparameters before it goes to the test data for final assessment.

## Question 7

Describe how backwards selection works.

__My Solution__

Backwards selection is when your model includes all predictors from the offset
and you remove the predictors that decrease r-squared the least until you land 
on an optimal model you are happy with. For this method the number of predictors must not 
exceed the number of samples in your data, or it will not work. Also, once a 
predictor is removed it is removed permanently.

## Question 8

Describe how best subset selection works.

__My Solution__

Best subset selection is the most computationally intensive out of the three
methods (forward, backward being the other two). It allows you to perform an
exhaustive search on _all_ possible combinations of models up to a designated
number of predictors you specify. Once complete, it will rank all possible
models based on their respective r-squared scores, from greatest to least.

