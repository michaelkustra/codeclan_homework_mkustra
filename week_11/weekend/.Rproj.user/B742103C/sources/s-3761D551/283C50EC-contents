---
title: "Clustering Lab"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Use k-means clustering to investigate potential relationships in the dataset `students_big` from the `CodeClanData` library. 

# Part a

  * We've been asked to create clusters of students for the answers to the 'importance' questions (where students rate how important they find certain topics on a scale from 0 to 1000, the higher the number the more important they think the issue is).
  
  <br> 
  
  * If we are to use the mathematical methods (elbow etc.) what would your optimal value of k been? Do you think the data seems well suited to cluster. 
<br> 


```{r, message = FALSE, warning = FALSE}
library(janitor)
library(fastDummies)
library(broom)
library(tidyverse)
library(factoextra)
```



Load data and perform same cleaning as in hierarchical clustering lesson: 

```{r, message = FALSE, warning = FALSE}
library(CodeClanData)

head(students_big)
```

Select the variables that contain information on how important the students rate certain issues (i.e. the columns that start with 'importance'). 

```{r}
students_big_subset <- students_big %>%
  select(starts_with("importance"))
```


Scale and check.

```{r}
students_big_scale <- students_big_subset %>% 
                      mutate_all(scale)

students_big_scale %>%
  as_tibble() %>%
  pivot_longer(cols = starts_with("importance"), 
                                  names_to = "type", 
               values_to = "value") %>% #convert data to long format
  group_by(type) %>%
  summarise(mean = round(mean(value)), 
            sd = sd(value))


```

Check each of the methods of choosing k: 

1. Elbow method   

```{r}
library(factoextra)
fviz_nbclust(students_big_scale, 
             kmeans, 
             method = "wss", 
             nstart = 25)
```

This graph is fairly smooth curve - rather than a defined kink. 

<br> 

2. Silhouette coefficient 

```{r}
fviz_nbclust(students_big_scale, kmeans, method = "silhouette", nstart = 25)
```

Silhouette method is giving k = 2 but the value for k = 3 is very close to the silhouette width for k = 2, so very close margin one of these values of k may have been picked as optimal. 

<br> 

3. Gap statistic

```{r}
fviz_nbclust(students_big_scale, kmeans, method = "gap_stat") #would put nstart=25 if had more computing power
```

This gives a result of k = 5. 

We get quite different results for each of the methods and for elbow and silhouette many values of k give quite similar results. When there isn't a clear optimal value of k, and where you don't have an intuitive value of k before hand, can be a sign data is not well suited for k-means clustering. 


<br> 


# Part b

  * We have been asked to cluster the variables which are asking about environmental questions (`importance_reducing_pollution`, `importance_recycling_rubbish`, `importance_conserving_water`, `importance_saving_enery`) into 2 clusters because the school wants to split the students into 2 groups to target with specific learning/lessons about environmental issues. 
  
  <br>
  
  * Once you have set up the 2 clusters visualise a combination of the variables split by the clusters (e.g. plot `importance_reducing_pollution` vs `importance_recycling_rubbish` etc.) how might you describe these 2 groups in terms of characteristics on views on environmental issues back to the school?

Select the variables that contain information on environmental questions (`importance_reducing_pollution`, `importance_recycling_rubbish`, `importance_conserving_water`, `importance_saving_enery`) 

```{r}
students_enviro_scale <- students_big_scale %>%
  select(importance_reducing_pollution, importance_recycling_rubbish, importance_conserving_water, importance_saving_enery)
```


```{r}
clustered_students_enviro <- kmeans(students_enviro_scale, 2, nstart = 25)

clustered_students_enviro
```

Pull out the cluster means and sizes for your chosen number of clusters. 

```{r}
clustered_students_enviro$size
clustered_students_enviro$centers
```

Visualise the clusters. 

```{r}
clusters <- augment(clustered_students_enviro, students_big)

ggplot(clusters, aes(importance_reducing_pollution, importance_recycling_rubbish, colour = .cluster)) +
  geom_point() 
```

```{r}
ggplot(clusters, aes(importance_conserving_water, importance_reducing_pollution, colour = .cluster)) +
  geom_point() 
```

```{r}
ggplot(clusters, aes(importance_conserving_water, importance_reducing_pollution, colour = .cluster)) +
  geom_point() 
```

```{r}
ggplot(clusters, aes(importance_saving_enery, importance_reducing_pollution, colour = .cluster)) +
  geom_point() 
```

What are the average importance scores for each environmental topic split by cluster?

```{r}
clusters %>% 
  group_by(.cluster) %>%
  summarise(mean(importance_reducing_pollution), mean(importance_recycling_rubbish), mean(importance_conserving_water), mean(importance_saving_enery))
```

Comment on the results:

Judging from the visualisations and the average scores that cluster 1 are students that think environmental issues are very important compared to cluster 2, which on average find them low to moderately high important (as it's an average this may include people who find some environmental issues very important but others not so much). You may tailor the lessons on environmental issues based on these 2 groups of students. Although we would go back and say it looks like clustering is perhaps not so optimally suited suited to this data.