---
title: "Logistic Regression HW"
output: html_notebook
---

```{r}
# load libraries
library(tidyverse)
library(janitor)
library(modelr)
library(GGally)
library(ggfortify)
library(pROC)
library(caret)
library(leaps)
library(yardstick)
```


```{r}
# read in orange juice data and clean names
juice_data <- read_csv(here::here("data/orange_juice.csv")) %>% 
  clean_names()
```

```{r}
# change purchase to logic variable
cols <- c("store7", "store_id", "special_ch", "special_mm", "store")

juice_data_clean <- juice_data %>%
  mutate(mm_purchase = case_when(
    purchase == "MM" ~ TRUE,
    purchase == "CH" ~ FALSE
  ), .before = 2) %>% 
  select(-purchase, -weekof_purchase) %>% 
  mutate_at(cols, as.factor)

juice_data_clean %>% 
  distinct(store_id, store)

```

```{r}
# check trends for week of purchase for both oj types
temp_df <- as.data.frame(table(juice_data$purchase, juice_data$weekof_purchase))
colnames(temp_df) <- c("purchase", "week", "frequency")

ggplot(temp_df, aes(x = week, y = frequency, colour = purchase, 
                    group = purchase))+
  geom_point() +
  geom_line() +
  theme_bw() +
  theme(legend.position = "top", axis.text.x = element_text(angle = 90, vjust = 0.5))+
  labs(title = "Number of purchase by week")
```

__Interpretation__
From the above plot I am going to decide to remove `weekof_purchase` from my 
logistic model since there doesn't seem to be a huge difference between the
two types of OJ and week of purchase. 

```{r}
# check for aliased variables
alias(mm_purchase ~ ., data = juice_data_clean)
```

__Interpretation__

Based on the alias output above, I will remove the following variables:

* price_ch
* price_mm
* disc_mm
* disc_ch
* store7
* store

```{r}
# remove aliased variables 
juice_data_clean <- juice_data_clean %>% 
  select(-c(price_ch, price_mm, disc_ch, disc_mm, store7, store))
```

```{r}
# pairs plot for multi-collinearity
ggpairs(juice_data_clean, columns = c(1, 6:11))
```

__Interpretation__

Following predictors stand out to me as promising from the pairs plot:

* loyal_ch (customer brand loyalty score for citrus hill)
* list_price_diff (list price of MM less list price of CH)
* store_id (id of each store in data set)

Also there could be issues with multi-collinearity due to the high positive
correlations between several of the predictors, I will remove the following:

* price_diff
* pct_disc_mm
* pct_disc_ch

```{r}
# remove variables to combat multicollinearity & recheck pairs plot
juice_data_clean <- juice_data_clean %>%
  select(-c(price_diff, pct_disc_mm, pct_disc_ch))

ggpairs(juice_data_clean)
```


```{r}
# split data into test/train sets
n_data <- nrow(juice_data_clean)

test_index <- sample(1:n_data, size = n_data * 0.2)

juice_test <- slice(juice_data_clean, test_index)

juice_train <- slice(juice_data_clean, -test_index)
```

```{r}
# check test/train table ratios
juice_train %>% 
  tabyl(mm_purchase)

juice_test %>% 
  tabyl(mm_purchase)
```

__Interpretation__

Ratios from the test and train data splits appear balanced, no issues here.

```{r}
# first logistic model
juice_log_model_1a <- glm(mm_purchase ~ loyal_ch + list_price_diff + store_id,
                          data = juice_train, 
                          family = binomial(link = 'logit'))
```

```{r}
# summary of logistic model
summary(juice_log_model_1a)
```

```{r}
# binomial anova using chi-square method to determine significance
anova(juice_log_model_1a, test = "Chisq")
```

__Interpretation__

This binomial ANOVA shows that each of the three current predictors are
significant, since their respective p-values are all < 0.05.

```{r}
# add prediction to model on train set
juice_log_model_1a_pred <- juice_train %>%
  add_predictions(juice_log_model_1a, type = "response")

# create roc object from model with predictions
roc_juice_1a <- juice_log_model_1a_pred %>%
  roc(response = mm_purchase, predictor = pred)

# calculate area under the curve for this model
auc(roc_juice_1a)
```

__Interpretation__

88.7% of the area is under our ROC curve, the bigger the percentage, the better.
This will serve as my baseline measure of performance.

```{r}
# reg subsets to help choose optimal number of variables
reg_summary <- summary(regsubsets(mm_purchase ~ ., data = juice_train,
                                  nvmax = 9))

summary(reg_summary)
```

```{r}
# choose optimal number of predictors for logistic model through subsets plots
par(mfrow = c(2,2))

plot(reg_summary$rss, xlab = "Number of variables", ylab = "RSS",
     type = "l")

plot(reg_summary$adjr2, xlab = "Number of variables", ylab = "Adjusted RSq",
     type = "l")

points(6, reg_summary$adjr2[6], col = "red", cex = 2, pch = 10)

plot(reg_summary$cp, xlab = "Number of variables", ylab = "Cp",
     type = "l")

points(3, reg_summary$cp[3], col = "red", cex = 2, pch = 10)

plot(reg_summary$bic, xlab = "Number of variables", ylab = "BIC",
     type = "l")

points(2, reg_summary$bic[2], col = "red", cex = 2, pch = 10)

reg_summary$cp

```


__Interpretation__

Used in conjunction, the above plots can give a good idea of the optimal
number of predictors that should be included in my model.

We would like to minimise Residual Sum of Squares (RSS) and can see this
decreases at a slower rate from 3 variables onwards.

We would like an adjusted r-squared as great as possible, this appears to 
shallow off after 3/4 variables. Meaning after 4 variables we are not getting
much extra explaining power in our model.

Much like RSS, we would like to minimise Bayesian Information Criterion (BIC)
which penalises extra variables and over-fitting in the model. It is clear that
from 4 variables on that it starts to increase (too many cooks spoiling the
broth) and has marked 2 as an optimal number.

CP is actually Mallows' Cp statistic which estimates the size of the bias that 
is introduced into the predicted responses by having an underspecified model.
It helps to show an over-fit model if it is too large. Ideally we want it to be
small and close to the number of predictors in the model plus the constant.
This is at it's lowest at 4 variables, meaning a model with 4 could provide
the most unbiased model.

```{r}
# which could/should be includded
summary(reg_summary$which)[3,]
```

```{r}
juice_log_model_1b <- glm(mm_purchase ~ loyal_ch + list_price_diff + store_id 
                          + sale_price_mm,
                          data = juice_train, 
                          family = binomial(link = 'logit'))
```

```{r}
summary(juice_log_model_1b)
```

```{r}
# add prediction to model on train set
juice_log_model_1b_pred <- juice_train %>%
  add_predictions(juice_log_model_1b, type = "response")

# create roc object from model with predictions
roc_juice_1b <- juice_log_model_1b_pred %>%
  roc(response = mm_purchase, predictor = pred)

# calculate area under the curve for this model
auc(roc_juice_1b)
```

__Interpretation__

Adding `sale_price_mm` hasn't done much for the ROC AUC, which is now 89.3%.
Previously 88.2%

```{r}
juice_log_model_1a_pred  %>% 
  mutate(model_pred = 1*(juice_log_model_1a$pred > 0.6) + 0,
                                 visit_binary = 1*(mm_purchase == "Yes") + 0)
```

```{r}
fitted_results <- predict(juice_log_model_1a,
                          new_data = subset(juice_test, select = c(2,3,4,5,6,7,8)), 
                          type = "response")

fitted_results <- ifelse(fitted_results > 0.5, 1, 0)

classify_error <- mean(fitted_results != juice_test$mm_purchase)

print(paste("Accuracy is", 1 - classify_error))
```

__Interpretation__

I don't think this is correct, however, accuracy appears to come out at 52.5%,
based on the test data set.